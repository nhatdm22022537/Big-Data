{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<a href=\"https://github.com/groda/big_data\"><div><img src=\"https://github.com/groda/big_data/blob/master/logo_bdb.png?raw=true\" align=right width=\"90\"></div></a>\n",
        "\n",
        "# MapReduce: A Primer with <code>Hello World!</code>\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "For this tutorial, we are going to download the core Hadoop distribution and run Hadoop in _local standalone mode_:\n",
        "\n",
        "> ❝ _By default, Hadoop is configured to run in a non-distributed mode, as a single Java process._ ❞\n",
        "\n",
        "(see [https://hadoop.apache.org/docs/stable/.../Standalone_Operation](https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/SingleCluster.html#Standalone_Operation))\n",
        "\n",
        "We are going to run a MapReduce job using MapReduce's [streaming application](https://hadoop.apache.org/docs/stable/hadoop-streaming/HadoopStreaming.html#Hadoop_Streaming). This is not to be confused with real-time streaming:\n",
        "\n",
        "> ❝ _Hadoop streaming is a utility that comes with the Hadoop distribution. The utility allows you to create and run Map/Reduce jobs with any executable or script as the mapper and/or the reducer._ ❞\n",
        "\n",
        "MapReduce streaming defaults to using [`IdentityMapper`](https://hadoop.apache.org/docs/stable/api/index.html) and [`IdentityReducer`](https://hadoop.apache.org/docs/stable/api/index.html), thus eliminating the need for explicit specification of a mapper or reducer. Finally, we show how to run a map-only job by setting `mapreduce.job.reduce` equal to $0$.\n",
        "\n",
        "Both input and output are standard files since Hadoop's default filesystem is the regular file system, as specified by the `fs.defaultFS` property in [core-default.xml](https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/core-default.xml)).\n"
      ],
      "metadata": {
        "id": "GzbmlR27wh6e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download core Hadoop"
      ],
      "metadata": {
        "id": "uUbM5R0GwwYw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "HADOOP_URL = \"https://dlcdn.apache.org/hadoop/common/stable/hadoop-3.4.0.tar.gz\"\n",
        "\n",
        "import requests\n",
        "import os\n",
        "import tarfile\n",
        "\n",
        "def download_and_extract_targz(url):\n",
        "    response = requests.get(url)\n",
        "    filename = url.rsplit('/', 1)[-1]\n",
        "    HADOOP_HOME = filename[:-7]\n",
        "    # set HADOOP_HOME environment variable\n",
        "    os.environ['HADOOP_HOME'] = HADOOP_HOME\n",
        "    if os.path.isdir(HADOOP_HOME):\n",
        "      print(\"Not downloading, Hadoop folder {} already exists\".format(HADOOP_HOME))\n",
        "      return\n",
        "    if response.status_code == 200:\n",
        "        with open(filename, 'wb') as file:\n",
        "            file.write(response.content)\n",
        "        with tarfile.open(filename, 'r:gz') as tar_ref:\n",
        "            extract_path = tar_ref.extractall(path='.')\n",
        "            # Get the names of all members (files and directories) in the archive\n",
        "            all_members = tar_ref.getnames()\n",
        "            # If there is a top-level directory, get its name\n",
        "            if all_members:\n",
        "              top_level_directory = all_members[0]\n",
        "              print(f\"ZIP file downloaded and extracted successfully. Contents saved at: {top_level_directory}\")\n",
        "    else:\n",
        "        print(f\"Failed to download ZIP file. Status code: {response.status_code}\")\n",
        "\n",
        "\n",
        "download_and_extract_targz(HADOOP_URL)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jDgQtQlzw8bL",
        "outputId": "d4905a5c-c648-4986-93b6-b8c8add4dc33"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ZIP file downloaded and extracted successfully. Contents saved at: hadoop-3.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Set environment variables"
      ],
      "metadata": {
        "id": "3yvb5cw9xEbh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set `HADOOP_HOME` and `PATH`"
      ],
      "metadata": {
        "id": "u6lkrz1dxIiO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# HADOOP_HOME was set earlier when downloading Hadoop distribution\n",
        "print(\"HADOOP_HOME is {}\".format(os.environ['HADOOP_HOME']))\n",
        "\n",
        "os.environ['PATH'] = ':'.join([os.path.join(os.environ['HADOOP_HOME'], 'bin'), os.environ['PATH']])\n",
        "print(\"PATH is {}\".format(os.environ['PATH']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s7maAwaFxBT_",
        "outputId": "580bb557-409b-42ef-bed2-7749a09431ad"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HADOOP_HOME is hadoop-3.4.0\n",
            "PATH is hadoop-3.4.0/bin:/opt/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/tools/node/bin:/tools/google-cloud-sdk/bin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set `JAVA_HOME`\n",
        "\n",
        "While Java is readily available on Google Colab, we consider the broader scenario of an Ubuntu machine. In this case, we ensure compatibility by installing Java, specifically opting for the `openjdk-19-jre-headless` version."
      ],
      "metadata": {
        "id": "4kzJ8cNoxPyK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "\n",
        "# set variable JAVA_HOME (install Java if necessary)\n",
        "def is_java_installed():\n",
        "    os.environ['JAVA_HOME'] = os.path.realpath(shutil.which(\"java\")).split('/bin')[0]\n",
        "    return os.environ['JAVA_HOME']\n",
        "\n",
        "def install_java():\n",
        "    # Uncomment and modify the desired version\n",
        "    # java_version= 'openjdk-11-jre-headless'\n",
        "    # java_version= 'default-jre'\n",
        "    # java_version= 'openjdk-17-jre-headless'\n",
        "    # java_version= 'openjdk-18-jre-headless'\n",
        "    java_version= 'openjdk-19-jre-headless'\n",
        "\n",
        "    print(f\"Java not found. Installing {java_version} ... (this might take a while)\")\n",
        "    try:\n",
        "        cmd = f\"apt install -y {java_version}\"\n",
        "        subprocess_output = subprocess.run(cmd, shell=True, check=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)\n",
        "        stdout_result = subprocess_output.stdout\n",
        "        # Process the results as needed\n",
        "        print(\"Done installing Java {}\".format(java_version))\n",
        "        os.environ['JAVA_HOME'] = os.path.realpath(shutil.which(\"java\")).split('/bin')[0]\n",
        "        print(\"JAVA_HOME is {}\".format(os.environ['JAVA_HOME']))\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        # Handle the error if the command returns a non-zero exit code\n",
        "        print(\"Command failed with return code {}\".format(e.returncode))\n",
        "        print(\"stdout: {}\".format(e.stdout))\n",
        "\n",
        "# Install Java if not available\n",
        "if is_java_installed():\n",
        "    print(\"Java is already installed: {}\".format(os.environ['JAVA_HOME']))\n",
        "else:\n",
        "    print(\"Installing Java\")\n",
        "    install_java()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SauFHVPOxL-Y",
        "outputId": "8328fe7e-edf1-40bc-9f64-c748ca70da5f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Java is already installed: /usr/lib/jvm/java-11-openjdk-amd64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run a MapReduce job with Hadoop streaming"
      ],
      "metadata": {
        "id": "6HFPVX84xbNd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create a file\n",
        "\n",
        "Write the string\"Hello, World!\" to a local file.<p>**Note:** you will be writing to the file `./hello.txt` in your current directory (denoted by `./`)."
      ],
      "metadata": {
        "id": "_yVa55X1xmOb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!echo \"Hello, World!\">./hello.txt"
      ],
      "metadata": {
        "id": "9Jz7mJkcxYxw"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Launch the MapReduce \"Hello, World!\" application\n",
        "\n",
        "Since the default filesystem is the local filesystem (as opposed to HDFS) we do not need to upload the local file `hello.txt` to HDFS.\n",
        "\n",
        "Run a MapReduce job with `/bin/cat` as a mapper and no reducer.\n",
        "\n",
        "**Note:** the first step of removing the output directory is necessary because MapReduce does not overwrite data folders by design."
      ],
      "metadata": {
        "id": "zSh_Kr5Bxvst"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "hdfs dfs -rm -r my_output\n",
        "\n",
        "mapred streaming \\\n",
        "    -input hello.txt \\\n",
        "    -output my_output \\\n",
        "    -mapper '/bin/cat'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nb5JryK9xpPA",
        "outputId": "0fc16c82-ee4e-49a4-e03f-2f9af81b90e1"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "rm: `my_output': No such file or directory\n",
            "2024-04-23 21:03:47,299 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
            "2024-04-23 21:03:47,627 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
            "2024-04-23 21:03:47,628 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n",
            "2024-04-23 21:03:47,666 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2024-04-23 21:03:48,020 INFO mapred.FileInputFormat: Total input files to process : 1\n",
            "2024-04-23 21:03:48,050 INFO mapreduce.JobSubmitter: number of splits:1\n",
            "2024-04-23 21:03:48,508 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1478803226_0001\n",
            "2024-04-23 21:03:48,508 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
            "2024-04-23 21:03:48,737 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
            "2024-04-23 21:03:48,739 INFO mapreduce.Job: Running job: job_local1478803226_0001\n",
            "2024-04-23 21:03:48,748 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
            "2024-04-23 21:03:48,750 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
            "2024-04-23 21:03:48,758 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2024-04-23 21:03:48,758 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2024-04-23 21:03:48,819 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
            "2024-04-23 21:03:48,826 INFO mapred.LocalJobRunner: Starting task: attempt_local1478803226_0001_m_000000_0\n",
            "2024-04-23 21:03:48,867 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2024-04-23 21:03:48,870 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2024-04-23 21:03:48,898 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2024-04-23 21:03:48,910 INFO mapred.MapTask: Processing split: file:/content/hello.txt:0+14\n",
            "2024-04-23 21:03:48,951 INFO mapred.MapTask: numReduceTasks: 1\n",
            "2024-04-23 21:03:49,044 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
            "2024-04-23 21:03:49,045 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
            "2024-04-23 21:03:49,045 INFO mapred.MapTask: soft limit at 83886080\n",
            "2024-04-23 21:03:49,045 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
            "2024-04-23 21:03:49,045 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
            "2024-04-23 21:03:49,050 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
            "2024-04-23 21:03:49,056 INFO streaming.PipeMapRed: PipeMapRed exec [/bin/cat]\n",
            "2024-04-23 21:03:49,062 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
            "2024-04-23 21:03:49,064 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
            "2024-04-23 21:03:49,065 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
            "2024-04-23 21:03:49,065 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
            "2024-04-23 21:03:49,066 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
            "2024-04-23 21:03:49,066 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
            "2024-04-23 21:03:49,068 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
            "2024-04-23 21:03:49,069 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
            "2024-04-23 21:03:49,069 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
            "2024-04-23 21:03:49,069 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
            "2024-04-23 21:03:49,070 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
            "2024-04-23 21:03:49,071 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
            "2024-04-23 21:03:49,094 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2024-04-23 21:03:49,104 INFO streaming.PipeMapRed: Records R/W=1/1\n",
            "2024-04-23 21:03:49,104 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2024-04-23 21:03:49,105 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2024-04-23 21:03:49,109 INFO mapred.LocalJobRunner: \n",
            "2024-04-23 21:03:49,110 INFO mapred.MapTask: Starting flush of map output\n",
            "2024-04-23 21:03:49,110 INFO mapred.MapTask: Spilling map output\n",
            "2024-04-23 21:03:49,110 INFO mapred.MapTask: bufstart = 0; bufend = 15; bufvoid = 104857600\n",
            "2024-04-23 21:03:49,110 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26214396(104857584); length = 1/6553600\n",
            "2024-04-23 21:03:49,121 INFO mapred.MapTask: Finished spill 0\n",
            "2024-04-23 21:03:49,157 INFO mapred.Task: Task:attempt_local1478803226_0001_m_000000_0 is done. And is in the process of committing\n",
            "2024-04-23 21:03:49,162 INFO mapred.LocalJobRunner: Records R/W=1/1\n",
            "2024-04-23 21:03:49,162 INFO mapred.Task: Task 'attempt_local1478803226_0001_m_000000_0' done.\n",
            "2024-04-23 21:03:49,171 INFO mapred.Task: Final Counters for attempt_local1478803226_0001_m_000000_0: Counters: 17\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=141914\n",
            "\t\tFILE: Number of bytes written=857635\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=1\n",
            "\t\tMap output records=1\n",
            "\t\tMap output bytes=15\n",
            "\t\tMap output materialized bytes=23\n",
            "\t\tInput split bytes=75\n",
            "\t\tCombine input records=0\n",
            "\t\tSpilled Records=1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=22\n",
            "\t\tTotal committed heap usage (bytes)=381681664\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=14\n",
            "2024-04-23 21:03:49,172 INFO mapred.LocalJobRunner: Finishing task: attempt_local1478803226_0001_m_000000_0\n",
            "2024-04-23 21:03:49,173 INFO mapred.LocalJobRunner: map task executor complete.\n",
            "2024-04-23 21:03:49,179 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
            "2024-04-23 21:03:49,180 INFO mapred.LocalJobRunner: Starting task: attempt_local1478803226_0001_r_000000_0\n",
            "2024-04-23 21:03:49,191 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2024-04-23 21:03:49,192 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2024-04-23 21:03:49,192 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2024-04-23 21:03:49,200 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@7de9423d\n",
            "2024-04-23 21:03:49,202 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2024-04-23 21:03:49,228 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=2382574336, maxSingleShuffleLimit=595643584, mergeThreshold=1572499072, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
            "2024-04-23 21:03:49,236 INFO reduce.EventFetcher: attempt_local1478803226_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
            "2024-04-23 21:03:49,285 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1478803226_0001_m_000000_0 decomp: 19 len: 23 to MEMORY\n",
            "2024-04-23 21:03:49,291 INFO reduce.InMemoryMapOutput: Read 19 bytes from map-output for attempt_local1478803226_0001_m_000000_0\n",
            "2024-04-23 21:03:49,294 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 19, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->19\n",
            "2024-04-23 21:03:49,301 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
            "2024-04-23 21:03:49,302 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2024-04-23 21:03:49,303 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
            "2024-04-23 21:03:49,320 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2024-04-23 21:03:49,320 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 3 bytes\n",
            "2024-04-23 21:03:49,322 INFO reduce.MergeManagerImpl: Merged 1 segments, 19 bytes to disk to satisfy reduce memory limit\n",
            "2024-04-23 21:03:49,323 INFO reduce.MergeManagerImpl: Merging 1 files, 23 bytes from disk\n",
            "2024-04-23 21:03:49,323 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
            "2024-04-23 21:03:49,323 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2024-04-23 21:03:49,324 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 3 bytes\n",
            "2024-04-23 21:03:49,325 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2024-04-23 21:03:49,337 INFO mapred.Task: Task:attempt_local1478803226_0001_r_000000_0 is done. And is in the process of committing\n",
            "2024-04-23 21:03:49,340 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2024-04-23 21:03:49,341 INFO mapred.Task: Task attempt_local1478803226_0001_r_000000_0 is allowed to commit now\n",
            "2024-04-23 21:03:49,343 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1478803226_0001_r_000000_0' to file:/content/my_output\n",
            "2024-04-23 21:03:49,351 INFO mapred.LocalJobRunner: reduce > reduce\n",
            "2024-04-23 21:03:49,352 INFO mapred.Task: Task 'attempt_local1478803226_0001_r_000000_0' done.\n",
            "2024-04-23 21:03:49,353 INFO mapred.Task: Final Counters for attempt_local1478803226_0001_r_000000_0: Counters: 24\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=141992\n",
            "\t\tFILE: Number of bytes written=857685\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=1\n",
            "\t\tReduce shuffle bytes=23\n",
            "\t\tReduce input records=1\n",
            "\t\tReduce output records=1\n",
            "\t\tSpilled Records=1\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=381681664\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=27\n",
            "2024-04-23 21:03:49,357 INFO mapred.LocalJobRunner: Finishing task: attempt_local1478803226_0001_r_000000_0\n",
            "2024-04-23 21:03:49,358 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
            "2024-04-23 21:03:49,745 INFO mapreduce.Job: Job job_local1478803226_0001 running in uber mode : false\n",
            "2024-04-23 21:03:49,746 INFO mapreduce.Job:  map 100% reduce 100%\n",
            "2024-04-23 21:03:49,748 INFO mapreduce.Job: Job job_local1478803226_0001 completed successfully\n",
            "2024-04-23 21:03:49,756 INFO mapreduce.Job: Counters: 30\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=283906\n",
            "\t\tFILE: Number of bytes written=1715320\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=1\n",
            "\t\tMap output records=1\n",
            "\t\tMap output bytes=15\n",
            "\t\tMap output materialized bytes=23\n",
            "\t\tInput split bytes=75\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=1\n",
            "\t\tReduce shuffle bytes=23\n",
            "\t\tReduce input records=1\n",
            "\t\tReduce output records=1\n",
            "\t\tSpilled Records=2\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=22\n",
            "\t\tTotal committed heap usage (bytes)=763363328\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=14\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=27\n",
            "2024-04-23 21:03:49,756 INFO streaming.StreamJob: Output directory: my_output\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Verify the result\n",
        "\n",
        "If the job executed successfully, an empty file named `_SUCCESS` is expected to be present in the output directory `my_output`.\n",
        "\n",
        "Verify the success of the MapReduce job by checking for the presence of the `_SUCCESS` file."
      ],
      "metadata": {
        "id": "OB_fX9u5x55y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "\n",
        "echo \"Check if MapReduce job was successful\"\n",
        "hdfs dfs -test -e my_output/_SUCCESS\n",
        "if [ $? -eq 0 ]; then\n",
        "\techo \"_SUCCESS exists!\"\n",
        "fi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bnvEvYDfx2g4",
        "outputId": "30d22905-5e05-4f83-9e64-98e86bce8116"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Check if MapReduce job was successful\n",
            "_SUCCESS exists!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note:** `hdfs dfs -ls` is the same as `ls` since the default filesystem is the local filesystem."
      ],
      "metadata": {
        "id": "BLMnBh44x_YR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!hdfs dfs -ls my_output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ufAfmGUvx8jW",
        "outputId": "2415e7f9-4c30-4039-c83e-b7f0e2004a99"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 2 items\n",
            "-rw-r--r--   1 root root          0 2024-04-23 21:03 my_output/_SUCCESS\n",
            "-rw-r--r--   1 root root         15 2024-04-23 21:03 my_output/part-00000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -l my_output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZnKSahPzyCAn",
        "outputId": "ee828764-4f21-418f-af3f-d17013194e6c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 4\n",
            "-rw-r--r-- 1 root root 15 Apr 23 21:03 part-00000\n",
            "-rw-r--r-- 1 root root  0 Apr 23 21:03 _SUCCESS\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The actual output of the MapReduce job is contained in the file `part-00000` in the output directory."
      ],
      "metadata": {
        "id": "v9LmpcaMyG23"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cat my_output/part-00000"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eL-Clat5yD8I",
        "outputId": "1a487479-763d-4f7c-9aab-cb7a7c8aca8a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello, World!\t\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MapReduce without specifying mapper or reducer\n",
        "\n",
        "In the previous example, we have seen how to run a MapReduce job without specifying any reducer.\n",
        "\n",
        "Since the only required options for `mapred streaming` are `input` and `output`, we can also run a MapReduce job without specifying a mapper."
      ],
      "metadata": {
        "id": "AmpHr_HyyMnM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mapred streaming -h"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZPWL1AiXyJac",
        "outputId": "425de331-924a-4547-f1bc-b6be3b8a4363"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-04-23 21:03:56,760 ERROR streaming.StreamJob: Unrecognized option: -h\n",
            "Usage: $HADOOP_HOME/bin/hadoop jar hadoop-streaming.jar [options]\n",
            "Options:\n",
            "  -input          <path> DFS input file(s) for the Map step.\n",
            "  -output         <path> DFS output directory for the Reduce step.\n",
            "  -mapper         <cmd|JavaClassName> Optional. Command to be run as mapper.\n",
            "  -combiner       <cmd|JavaClassName> Optional. Command to be run as combiner.\n",
            "  -reducer        <cmd|JavaClassName> Optional. Command to be run as reducer.\n",
            "  -file           <file> Optional. File/dir to be shipped in the Job jar file.\n",
            "                  Deprecated. Use generic option \"-files\" instead.\n",
            "  -inputformat    <TextInputFormat(default)|SequenceFileAsTextInputFormat|JavaClassName>\n",
            "                  Optional. The input format class.\n",
            "  -outputformat   <TextOutputFormat(default)|JavaClassName>\n",
            "                  Optional. The output format class.\n",
            "  -partitioner    <JavaClassName>  Optional. The partitioner class.\n",
            "  -numReduceTasks <num> Optional. Number of reduce tasks.\n",
            "  -inputreader    <spec> Optional. Input recordreader spec.\n",
            "  -cmdenv         <n>=<v> Optional. Pass env.var to streaming commands.\n",
            "  -mapdebug       <cmd> Optional. To run this script when a map task fails.\n",
            "  -reducedebug    <cmd> Optional. To run this script when a reduce task fails.\n",
            "  -io             <identifier> Optional. Format to use for input to and output\n",
            "                  from mapper/reducer commands\n",
            "  -lazyOutput     Optional. Lazily create Output.\n",
            "  -background     Optional. Submit the job and don't wait till it completes.\n",
            "  -verbose        Optional. Print verbose output.\n",
            "  -info           Optional. Print detailed usage.\n",
            "  -help           Optional. Print help message.\n",
            "\n",
            "Generic options supported are:\n",
            "-conf <configuration file>        specify an application configuration file\n",
            "-D <property=value>               define a value for a given property\n",
            "-fs <file:///|hdfs://namenode:port> specify default filesystem URL to use, overrides 'fs.defaultFS' property from configurations.\n",
            "-jt <local|resourcemanager:port>  specify a ResourceManager\n",
            "-files <file1,...>                specify a comma-separated list of files to be copied to the map reduce cluster\n",
            "-libjars <jar1,...>               specify a comma-separated list of jar files to be included in the classpath\n",
            "-archives <archive1,...>          specify a comma-separated list of archives to be unarchived on the compute machines\n",
            "\n",
            "The general command line syntax is:\n",
            "command [genericOptions] [commandOptions]\n",
            "\n",
            "\n",
            "For more details about these options:\n",
            "Use $HADOOP_HOME/bin/hadoop jar hadoop-streaming.jar -info\n",
            "\n",
            "Try -help for more information\n",
            "Streaming Command Failed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "hdfs dfs -rm -r my_output\n",
        "\n",
        "mapred streaming \\\n",
        "    -input hello.txt \\\n",
        "    -output my_output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5H2MkIUPyQc2",
        "outputId": "3b0dbfe6-f25c-4ee0-b245-dd8ddceba319"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Deleted my_output\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-04-23 21:03:58,954 INFO Configuration.deprecation: io.bytes.per.checksum is deprecated. Instead, use dfs.bytes-per-checksum\n",
            "2024-04-23 21:04:01,525 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
            "2024-04-23 21:04:01,712 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
            "2024-04-23 21:04:01,712 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n",
            "2024-04-23 21:04:01,738 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2024-04-23 21:04:02,050 INFO mapred.FileInputFormat: Total input files to process : 1\n",
            "2024-04-23 21:04:02,088 INFO mapreduce.JobSubmitter: number of splits:1\n",
            "2024-04-23 21:04:02,457 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1929686774_0001\n",
            "2024-04-23 21:04:02,457 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
            "2024-04-23 21:04:02,709 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
            "2024-04-23 21:04:02,711 INFO mapreduce.Job: Running job: job_local1929686774_0001\n",
            "2024-04-23 21:04:02,728 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
            "2024-04-23 21:04:02,730 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
            "2024-04-23 21:04:02,742 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2024-04-23 21:04:02,742 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2024-04-23 21:04:02,803 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
            "2024-04-23 21:04:02,809 INFO mapred.LocalJobRunner: Starting task: attempt_local1929686774_0001_m_000000_0\n",
            "2024-04-23 21:04:02,846 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2024-04-23 21:04:02,849 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2024-04-23 21:04:02,880 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2024-04-23 21:04:02,893 INFO mapred.MapTask: Processing split: file:/content/hello.txt:0+14\n",
            "2024-04-23 21:04:02,911 INFO mapred.MapTask: numReduceTasks: 1\n",
            "2024-04-23 21:04:03,003 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
            "2024-04-23 21:04:03,003 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
            "2024-04-23 21:04:03,003 INFO mapred.MapTask: soft limit at 83886080\n",
            "2024-04-23 21:04:03,003 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
            "2024-04-23 21:04:03,003 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
            "2024-04-23 21:04:03,008 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
            "2024-04-23 21:04:03,017 INFO mapred.LocalJobRunner: \n",
            "2024-04-23 21:04:03,018 INFO mapred.MapTask: Starting flush of map output\n",
            "2024-04-23 21:04:03,018 INFO mapred.MapTask: Spilling map output\n",
            "2024-04-23 21:04:03,018 INFO mapred.MapTask: bufstart = 0; bufend = 22; bufvoid = 104857600\n",
            "2024-04-23 21:04:03,018 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26214396(104857584); length = 1/6553600\n",
            "2024-04-23 21:04:03,027 INFO mapred.MapTask: Finished spill 0\n",
            "2024-04-23 21:04:03,059 INFO mapred.Task: Task:attempt_local1929686774_0001_m_000000_0 is done. And is in the process of committing\n",
            "2024-04-23 21:04:03,063 INFO mapred.LocalJobRunner: file:/content/hello.txt:0+14\n",
            "2024-04-23 21:04:03,063 INFO mapred.Task: Task 'attempt_local1929686774_0001_m_000000_0' done.\n",
            "2024-04-23 21:04:03,072 INFO mapred.Task: Final Counters for attempt_local1929686774_0001_m_000000_0: Counters: 17\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=141914\n",
            "\t\tFILE: Number of bytes written=855525\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=1\n",
            "\t\tMap output records=1\n",
            "\t\tMap output bytes=22\n",
            "\t\tMap output materialized bytes=30\n",
            "\t\tInput split bytes=75\n",
            "\t\tCombine input records=0\n",
            "\t\tSpilled Records=1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=15\n",
            "\t\tTotal committed heap usage (bytes)=393216000\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=14\n",
            "2024-04-23 21:04:03,072 INFO mapred.LocalJobRunner: Finishing task: attempt_local1929686774_0001_m_000000_0\n",
            "2024-04-23 21:04:03,073 INFO mapred.LocalJobRunner: map task executor complete.\n",
            "2024-04-23 21:04:03,078 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
            "2024-04-23 21:04:03,081 INFO mapred.LocalJobRunner: Starting task: attempt_local1929686774_0001_r_000000_0\n",
            "2024-04-23 21:04:03,093 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2024-04-23 21:04:03,093 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2024-04-23 21:04:03,094 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2024-04-23 21:04:03,101 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@3118b871\n",
            "2024-04-23 21:04:03,104 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2024-04-23 21:04:03,131 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=2382574336, maxSingleShuffleLimit=595643584, mergeThreshold=1572499072, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
            "2024-04-23 21:04:03,137 INFO reduce.EventFetcher: attempt_local1929686774_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
            "2024-04-23 21:04:03,190 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1929686774_0001_m_000000_0 decomp: 26 len: 30 to MEMORY\n",
            "2024-04-23 21:04:03,195 INFO reduce.InMemoryMapOutput: Read 26 bytes from map-output for attempt_local1929686774_0001_m_000000_0\n",
            "2024-04-23 21:04:03,200 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 26, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->26\n",
            "2024-04-23 21:04:03,204 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
            "2024-04-23 21:04:03,205 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2024-04-23 21:04:03,205 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
            "2024-04-23 21:04:03,214 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2024-04-23 21:04:03,214 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 16 bytes\n",
            "2024-04-23 21:04:03,216 INFO reduce.MergeManagerImpl: Merged 1 segments, 26 bytes to disk to satisfy reduce memory limit\n",
            "2024-04-23 21:04:03,218 INFO reduce.MergeManagerImpl: Merging 1 files, 30 bytes from disk\n",
            "2024-04-23 21:04:03,221 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
            "2024-04-23 21:04:03,221 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2024-04-23 21:04:03,225 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 16 bytes\n",
            "2024-04-23 21:04:03,226 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2024-04-23 21:04:03,255 INFO mapred.Task: Task:attempt_local1929686774_0001_r_000000_0 is done. And is in the process of committing\n",
            "2024-04-23 21:04:03,257 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2024-04-23 21:04:03,257 INFO mapred.Task: Task attempt_local1929686774_0001_r_000000_0 is allowed to commit now\n",
            "2024-04-23 21:04:03,260 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1929686774_0001_r_000000_0' to file:/content/my_output\n",
            "2024-04-23 21:04:03,268 INFO mapred.LocalJobRunner: reduce > reduce\n",
            "2024-04-23 21:04:03,268 INFO mapred.Task: Task 'attempt_local1929686774_0001_r_000000_0' done.\n",
            "2024-04-23 21:04:03,268 INFO mapred.Task: Final Counters for attempt_local1929686774_0001_r_000000_0: Counters: 24\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=142006\n",
            "\t\tFILE: Number of bytes written=855583\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=1\n",
            "\t\tReduce shuffle bytes=30\n",
            "\t\tReduce input records=1\n",
            "\t\tReduce output records=1\n",
            "\t\tSpilled Records=1\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=393216000\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=28\n",
            "2024-04-23 21:04:03,269 INFO mapred.LocalJobRunner: Finishing task: attempt_local1929686774_0001_r_000000_0\n",
            "2024-04-23 21:04:03,269 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
            "2024-04-23 21:04:03,726 INFO mapreduce.Job: Job job_local1929686774_0001 running in uber mode : false\n",
            "2024-04-23 21:04:03,727 INFO mapreduce.Job:  map 100% reduce 100%\n",
            "2024-04-23 21:04:03,729 INFO mapreduce.Job: Job job_local1929686774_0001 completed successfully\n",
            "2024-04-23 21:04:03,749 INFO mapreduce.Job: Counters: 30\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=283920\n",
            "\t\tFILE: Number of bytes written=1711108\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=1\n",
            "\t\tMap output records=1\n",
            "\t\tMap output bytes=22\n",
            "\t\tMap output materialized bytes=30\n",
            "\t\tInput split bytes=75\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=1\n",
            "\t\tReduce shuffle bytes=30\n",
            "\t\tReduce input records=1\n",
            "\t\tReduce output records=1\n",
            "\t\tSpilled Records=2\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=15\n",
            "\t\tTotal committed heap usage (bytes)=786432000\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=14\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=28\n",
            "2024-04-23 21:04:03,752 INFO streaming.StreamJob: Output directory: my_output\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Verify the result"
      ],
      "metadata": {
        "id": "v7Ks3e96yXuB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "\n",
        "echo \"Check if MapReduce job was successful\"\n",
        "hdfs dfs -test -e my_output/_SUCCESS\n",
        "if [ $? -eq 0 ]; then\n",
        "\techo \"_SUCCESS exists!\"\n",
        "fi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cWAXvG0_yThc",
        "outputId": "353a9e92-6823-42f3-facd-a3a13e01ed55"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Check if MapReduce job was successful\n",
            "_SUCCESS exists!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Show output"
      ],
      "metadata": {
        "id": "t40GgJ2Hya9P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cat my_output/part-00000"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I5APWEgoyaRS",
        "outputId": "108f8e62-7f0d-4d61-e9d0-6e4aaccafcf8"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\tHello, World!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "What happened here is that not having defined any mapper or reducer, the \"Identity\" mapper ([IdentityMapper](https://hadoop.apache.org/docs/stable/api/org/apache/hadoop/mapred/lib/IdentityMapper.html)) and reducer ([IdentityReducer](https://hadoop.apache.org/docs/stable/api/org/apache/hadoop/mapred/lib/IdentityReducer.html)) were used by default (see [Streaming command options](https://hadoop.apache.org/docs/stable/hadoop-streaming/HadoopStreaming.html#Streaming_Command_Options))."
      ],
      "metadata": {
        "id": "mzfaMVKqyjpC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run a map-only MapReduce job\n",
        "\n",
        "Not specifying mapper and reducer in the MapReduce job submission does not mean that MapReduce isn't going to run the mapper and reducer steps, it is simply going to use the Identity mapper and reducer.\n",
        "\n",
        "To run a MapReduce job _without_ reducer one needs to use the generic option\n",
        "\n",
        "    \\-D mapreduce.job.reduces=0\n",
        "\n",
        "(see [specifying map-only jobs](https://hadoop.apache.org/docs/stable/hadoop-streaming/HadoopStreaming.html#Specifying_Map-Only_Jobs))."
      ],
      "metadata": {
        "id": "lzIuWv7Myndc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "hdfs dfs -rm -r my_output\n",
        "\n",
        "mapred streaming \\\n",
        "    -D mapreduce.job.reduces=0 \\\n",
        "    -input hello.txt \\\n",
        "    -output my_output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OdwKWyVRye27",
        "outputId": "a99d86d2-e679-47c7-9c78-34504ac3e437"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Deleted my_output\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-04-23 21:04:08,624 INFO Configuration.deprecation: io.bytes.per.checksum is deprecated. Instead, use dfs.bytes-per-checksum\n",
            "2024-04-23 21:04:10,869 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
            "2024-04-23 21:04:11,098 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
            "2024-04-23 21:04:11,099 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n",
            "2024-04-23 21:04:11,132 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2024-04-23 21:04:11,438 INFO mapred.FileInputFormat: Total input files to process : 1\n",
            "2024-04-23 21:04:11,479 INFO mapreduce.JobSubmitter: number of splits:1\n",
            "2024-04-23 21:04:11,847 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local861832306_0001\n",
            "2024-04-23 21:04:11,847 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
            "2024-04-23 21:04:12,077 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
            "2024-04-23 21:04:12,079 INFO mapreduce.Job: Running job: job_local861832306_0001\n",
            "2024-04-23 21:04:12,093 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
            "2024-04-23 21:04:12,097 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
            "2024-04-23 21:04:12,108 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2024-04-23 21:04:12,108 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2024-04-23 21:04:12,189 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
            "2024-04-23 21:04:12,197 INFO mapred.LocalJobRunner: Starting task: attempt_local861832306_0001_m_000000_0\n",
            "2024-04-23 21:04:12,263 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2024-04-23 21:04:12,264 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2024-04-23 21:04:12,293 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2024-04-23 21:04:12,302 INFO mapred.MapTask: Processing split: file:/content/hello.txt:0+14\n",
            "2024-04-23 21:04:12,320 INFO mapred.MapTask: numReduceTasks: 0\n",
            "2024-04-23 21:04:12,343 INFO mapred.LocalJobRunner: \n",
            "2024-04-23 21:04:12,355 INFO mapred.Task: Task:attempt_local861832306_0001_m_000000_0 is done. And is in the process of committing\n",
            "2024-04-23 21:04:12,357 INFO mapred.LocalJobRunner: \n",
            "2024-04-23 21:04:12,357 INFO mapred.Task: Task attempt_local861832306_0001_m_000000_0 is allowed to commit now\n",
            "2024-04-23 21:04:12,367 INFO output.FileOutputCommitter: Saved output of task 'attempt_local861832306_0001_m_000000_0' to file:/content/my_output\n",
            "2024-04-23 21:04:12,368 INFO mapred.LocalJobRunner: file:/content/hello.txt:0+14\n",
            "2024-04-23 21:04:12,368 INFO mapred.Task: Task 'attempt_local861832306_0001_m_000000_0' done.\n",
            "2024-04-23 21:04:12,405 INFO mapred.Task: Final Counters for attempt_local861832306_0001_m_000000_0: Counters: 15\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=141914\n",
            "\t\tFILE: Number of bytes written=852049\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=1\n",
            "\t\tMap output records=1\n",
            "\t\tInput split bytes=75\n",
            "\t\tSpilled Records=0\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=425721856\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=14\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=28\n",
            "2024-04-23 21:04:12,405 INFO mapred.LocalJobRunner: Finishing task: attempt_local861832306_0001_m_000000_0\n",
            "2024-04-23 21:04:12,406 INFO mapred.LocalJobRunner: map task executor complete.\n",
            "2024-04-23 21:04:13,087 INFO mapreduce.Job: Job job_local861832306_0001 running in uber mode : false\n",
            "2024-04-23 21:04:13,090 INFO mapreduce.Job:  map 100% reduce 0%\n",
            "2024-04-23 21:04:13,093 INFO mapreduce.Job: Job job_local861832306_0001 completed successfully\n",
            "2024-04-23 21:04:13,104 INFO mapreduce.Job: Counters: 15\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=141914\n",
            "\t\tFILE: Number of bytes written=852049\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=1\n",
            "\t\tMap output records=1\n",
            "\t\tInput split bytes=75\n",
            "\t\tSpilled Records=0\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=425721856\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=14\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=28\n",
            "2024-04-23 21:04:13,104 INFO streaming.StreamJob: Output directory: my_output\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Verify the result"
      ],
      "metadata": {
        "id": "QZIE9yXOyyHJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!hdfs dfs -test -e my_output/_SUCCESS && cat my_output/part-00000"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Dt3tUI0yu5e",
        "outputId": "3097dd0f-4f59-474d-f52c-fc04bb9920a9"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\tHello, World!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Why a map-only application?\n",
        "\n",
        "The advantage of a map-only job is that the sorting and shuffling phases are skipped, so if you do not need that remember to specify `-D mapreduce.job.reduces=0 `.\n",
        "\n",
        "On the other hand, a MapReduce job even with the default `IdentityReducer` will deliver sorted results because the data passed from the mapper to the reducer always gets sorted.\n"
      ],
      "metadata": {
        "id": "hUGEUv99y3cM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Improved version of the MapReduce \"Hello, World!\" application\n",
        "\n",
        "Taking into account the previous considerations, here's a more efficient version of the 'Hello, World!' application that bypasses the shuffling and sorting step."
      ],
      "metadata": {
        "id": "FhVVFEdKzGcI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "hdfs dfs -rm -r my_output\n",
        "\n",
        "mapred streaming \\\n",
        "    -D mapreduce.job.reduces=0 \\\n",
        "    -input hello.txt \\\n",
        "    -output my_output \\\n",
        "    -mapper '/bin/cat'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jLgMXX2jy0vC",
        "outputId": "45fed446-40af-46af-dc1a-6de019ee28ca"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Deleted my_output\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-04-23 21:04:17,724 INFO Configuration.deprecation: io.bytes.per.checksum is deprecated. Instead, use dfs.bytes-per-checksum\n",
            "2024-04-23 21:04:20,584 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
            "2024-04-23 21:04:20,796 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
            "2024-04-23 21:04:20,796 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n",
            "2024-04-23 21:04:20,819 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2024-04-23 21:04:21,094 INFO mapred.FileInputFormat: Total input files to process : 1\n",
            "2024-04-23 21:04:21,132 INFO mapreduce.JobSubmitter: number of splits:1\n",
            "2024-04-23 21:04:21,433 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local642727353_0001\n",
            "2024-04-23 21:04:21,433 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
            "2024-04-23 21:04:21,719 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
            "2024-04-23 21:04:21,722 INFO mapreduce.Job: Running job: job_local642727353_0001\n",
            "2024-04-23 21:04:21,735 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
            "2024-04-23 21:04:21,739 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
            "2024-04-23 21:04:21,747 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2024-04-23 21:04:21,748 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2024-04-23 21:04:21,817 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
            "2024-04-23 21:04:21,826 INFO mapred.LocalJobRunner: Starting task: attempt_local642727353_0001_m_000000_0\n",
            "2024-04-23 21:04:21,867 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2024-04-23 21:04:21,870 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2024-04-23 21:04:21,908 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2024-04-23 21:04:21,921 INFO mapred.MapTask: Processing split: file:/content/hello.txt:0+14\n",
            "2024-04-23 21:04:21,939 INFO mapred.MapTask: numReduceTasks: 0\n",
            "2024-04-23 21:04:21,953 INFO streaming.PipeMapRed: PipeMapRed exec [/bin/cat]\n",
            "2024-04-23 21:04:21,964 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
            "2024-04-23 21:04:21,970 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
            "2024-04-23 21:04:21,970 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
            "2024-04-23 21:04:21,970 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
            "2024-04-23 21:04:21,971 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
            "2024-04-23 21:04:21,972 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
            "2024-04-23 21:04:21,974 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
            "2024-04-23 21:04:21,975 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
            "2024-04-23 21:04:21,975 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
            "2024-04-23 21:04:21,976 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
            "2024-04-23 21:04:21,976 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
            "2024-04-23 21:04:21,977 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
            "2024-04-23 21:04:22,004 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2024-04-23 21:04:22,020 INFO streaming.PipeMapRed: Records R/W=1/1\n",
            "2024-04-23 21:04:22,021 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2024-04-23 21:04:22,021 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2024-04-23 21:04:22,025 INFO mapred.LocalJobRunner: \n",
            "2024-04-23 21:04:22,042 INFO mapred.Task: Task:attempt_local642727353_0001_m_000000_0 is done. And is in the process of committing\n",
            "2024-04-23 21:04:22,044 INFO mapred.LocalJobRunner: \n",
            "2024-04-23 21:04:22,044 INFO mapred.Task: Task attempt_local642727353_0001_m_000000_0 is allowed to commit now\n",
            "2024-04-23 21:04:22,055 INFO output.FileOutputCommitter: Saved output of task 'attempt_local642727353_0001_m_000000_0' to file:/content/my_output\n",
            "2024-04-23 21:04:22,057 INFO mapred.LocalJobRunner: Records R/W=1/1\n",
            "2024-04-23 21:04:22,059 INFO mapred.Task: Task 'attempt_local642727353_0001_m_000000_0' done.\n",
            "2024-04-23 21:04:22,070 INFO mapred.Task: Final Counters for attempt_local642727353_0001_m_000000_0: Counters: 15\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=141914\n",
            "\t\tFILE: Number of bytes written=855001\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=1\n",
            "\t\tMap output records=1\n",
            "\t\tInput split bytes=75\n",
            "\t\tSpilled Records=0\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=364904448\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=14\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=27\n",
            "2024-04-23 21:04:22,070 INFO mapred.LocalJobRunner: Finishing task: attempt_local642727353_0001_m_000000_0\n",
            "2024-04-23 21:04:22,072 INFO mapred.LocalJobRunner: map task executor complete.\n",
            "2024-04-23 21:04:22,733 INFO mapreduce.Job: Job job_local642727353_0001 running in uber mode : false\n",
            "2024-04-23 21:04:22,736 INFO mapreduce.Job:  map 100% reduce 0%\n",
            "2024-04-23 21:04:22,739 INFO mapreduce.Job: Job job_local642727353_0001 completed successfully\n",
            "2024-04-23 21:04:22,755 INFO mapreduce.Job: Counters: 15\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=141914\n",
            "\t\tFILE: Number of bytes written=855001\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=1\n",
            "\t\tMap output records=1\n",
            "\t\tInput split bytes=75\n",
            "\t\tSpilled Records=0\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=364904448\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=14\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=27\n",
            "2024-04-23 21:04:22,756 INFO streaming.StreamJob: Output directory: my_output\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!hdfs dfs -test -e my_output/_SUCCESS && cat my_output/part-00000"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sa1UDPr6zKKw",
        "outputId": "592c6946-90fd-47b3-85b6-d0dde6a3d379"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello, World!\t\n"
          ]
        }
      ]
    }
  ]
}